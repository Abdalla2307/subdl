import os
import re
import cloudscraper
from bs4 import BeautifulSoup
from telegram.ext import Updater, MessageHandler, Filters, CommandHandler

TOKEN = os.getenv("TOKEN")
PROXY = os.getenv("PROXY")  # Ù…Ø«Ø§Ù„: http://144.125.164.158:8080

if not TOKEN:
    raise ValueError("TOKEN is missing. Add TOKEN in Heroku Config Vars.")

# Ø¬Ù‡Ù‘Ø² Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
proxies = None
if PROXY and PROXY.strip():
    p = PROXY.strip()
    # Ù„Ùˆ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙƒØªØ¨ IP:PORT ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† http://
    if not p.startswith("http://") and not p.startswith("https://"):
        p = "http://" + p
    proxies = {"http": p, "https": p}

# cloudscraper Ù„ØªØ¬Ø§ÙˆØ² Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ù…Ø§ÙŠØ§Øª
scraper = cloudscraper.create_scraper(
    browser={"browser": "chrome", "platform": "windows", "mobile": False}
)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari/537.36",
    "Accept-Language": "en-US,en;q=0.9,ar;q=0.8",
}

def start(update, context):
    update.message.reply_text("ğŸ¬ Ø§Ø¨Ø¹Øª Ø§Ø³Ù… Ø§Ù„ÙÙŠÙ„Ù…/Ø§Ù„Ù…Ø³Ù„Ø³Ù„ (Ù…Ø«Ø§Ù„: tenet)")

def google_find_subdl_pages(query: str, limit: int = 5):
    # Ù†Ø¨Ø­Ø« ÙÙŠ Ø¬ÙˆØ¬Ù„ Ø¹Ù† ØµÙØ­Ø§Øª subdl subtitle
    q = f'site:subdl.com/subtitle {query}'
    url = "https://www.google.com/search?q=" + re.sub(r"\s+", "+", q.strip())

    html = scraper.get(url, headers=HEADERS, proxies=proxies, timeout=25).text
    soup = BeautifulSoup(html, "html.parser")

    links = []
    for a in soup.select("a"):
        href = a.get("href") or ""
        # Ø¬ÙˆØ¬Ù„ Ø¨ÙŠØ­Ø·Ù‡Ø§ ØºØ§Ù„Ø¨Ù‹Ø§ /url?q=....
        m = re.search(r"(https://subdl\.com/subtitle/[^&]+)", href)
        if m:
            link = m.group(1)
            if link not in links:
                links.append(link)
        if len(links) >= limit:
            break
    return links

def extract_download_links(subtitle_page_url: str):
    html = scraper.get(subtitle_page_url, headers=HEADERS, proxies=proxies, timeout=25).text
    soup = BeautifulSoup(html, "html.parser")

    # SubDL Ø¨ÙŠÙƒÙˆÙ† ÙÙŠÙ‡ Ø±ÙˆØ§Ø¨Ø· download Ø¨Ø£Ø´ÙƒØ§Ù„ Ù…Ø®ØªÙ„ÙØ© Ø­Ø³Ø¨ Ø§Ù„ØµÙØ­Ø©
    links = []

    # Ø£ÙŠ Ù„ÙŠÙ†Ùƒ ÙÙŠÙ‡ download Ø£Ùˆ zip
    for a in soup.select("a"):
        href = a.get("href") or ""
        text = (a.get_text() or "").strip().lower()

        if not href:
            continue

        # Ø­ÙˆÙ‘Ù„ Ø§Ù„Ù†Ø³Ø¨ÙŠ Ù„Ù…Ø·Ù„Ù‚
        if href.startswith("/"):
            href_full = "https://subdl.com" + href
        else:
            href_full = href

        if ("download" in href.lower()) or ("zip" in href.lower()) or ("download" in text):
            if href_full not in links:
                links.append(href_full)

    return links

def handle_text(update, context):
    q = (update.message.text or "").strip()
    if not q:
        update.message.reply_text("Ø§ÙƒØªØ¨ Ø§Ø³Ù… ØµØ­ÙŠØ­.")
        return

    update.message.reply_text("â³ Ø¨Ø¯ÙˆÙ‘Ø±â€¦")

    try:
        pages = google_find_subdl_pages(q, limit=5)
        if not pages:
            update.message.reply_text("âŒ Ù…Ù„Ù‚ØªØ´ ØµÙØ­Ø§Øª ØªØ±Ø¬Ù…Ø© Ø¹Ù„Ù‰ SubDL (Ù…Ù…ÙƒÙ† Ø¬ÙˆØ¬Ù„/Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠ Ø­Ø§Ø¸Ø±ÙŠÙ†). Ø¬Ø±Ù‘Ø¨ Ø¨Ø±ÙˆÙƒØ³ÙŠ ØªØ§Ù†ÙŠ.")
            return

        # Ø§Ø®ØªØ§Ø± Ø£ÙˆÙ„ ØµÙØ­Ø© subtitle
        page = pages[0]
        dl_links = extract_download_links(page)

        msg = f"âœ… Ù„Ù‚ÙŠØª ØµÙØ­Ø© ØªØ±Ø¬Ù…Ø©:\n{page}\n"
        if dl_links:
            msg += "\nâ¬‡ï¸ Ø±ÙˆØ§Ø¨Ø· ØªØ­Ù…ÙŠÙ„ Ù…Ø­ØªÙ…Ù„Ø©:\n" + "\n".join(dl_links[:5])
        else:
            msg += "\nâš ï¸ Ù…Ù„Ù‚ØªØ´ Ø²Ø± ØªØ­Ù…ÙŠÙ„ ÙÙŠ Ø§Ù„ØµÙØ­Ø© Ø¯ÙŠ (Ø¬Ø±Ù‘Ø¨ ØªÙØªØ­ Ø§Ù„Ù„ÙŠÙ†Ùƒ Ø¨Ù†ÙØ³Ùƒ Ø£Ùˆ Ø¬Ø±Ù‘Ø¨ Ù†ØªÙŠØ¬Ø© ØªØ§Ù†ÙŠØ©)."

        update.message.reply_text(msg)

    except Exception as e:
        print("ERROR:", e)
        update.message.reply_text("âš ï¸ Ø­ØµÙ„ Ø®Ø·Ø£ (ØºØ§Ù„Ø¨Ù‹Ø§ Ø§Ù„Ø¨Ø±ÙˆÙƒØ³ÙŠ ÙˆØ§Ù‚Ø¹/Ø¨Ø·ÙŠØ¡). Ø¬Ø±Ù‘Ø¨ Ø¨Ø±ÙˆÙƒØ³ÙŠ ØªØ§Ù†ÙŠ.")

def main():
    updater = Updater(TOKEN, use_context=True)
    dp = updater.dispatcher

    dp.add_handler(CommandHandler("start", start))
    dp.add_handler(MessageHandler(Filters.text & ~Filters.command, handle_text))

    updater.start_polling()
    updater.idle()

if __name__ == "__main__":
    main()
